{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPafYg44JZAhuiNY2AY8nCb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# 目標網址\n","url = \"https://www.ithome.com.tw/news/152373\"\n","\n","# 發送請求\n","response = requests.get(url)\n","response.raise_for_status()  # 確保請求成功\n","\n","# 解析 HTML\n","soup = BeautifulSoup(response.text, 'html.parser')\n","\n","# 抓取文章標題\n","title = soup.find('h1', class_='page-header').get_text(strip=True)\n","\n","# 抓取文章內容\n","content_div = soup.find('div', class_='field-items')\n","paragraphs = content_div.find_all('p') if content_div else []\n","content = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n","\n","# 輸出結果\n","print(\"文章標題：\", title)\n","print(\"\\n文章內容：\")\n","print(content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IzY5CWMIqnnE","executionInfo":{"status":"ok","timestamp":1733282173116,"user_tz":-480,"elapsed":1495,"user":{"displayName":"呂彥欣","userId":"13178541323415113438"}},"outputId":"8334df2d-b248-4b74-a783-079371cc4b0a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["文章標題： 目標未來3～5年全集團100套系統上雲，國泰金控實現大規模上雲的關鍵策略\n","\n","文章內容：\n","\n"]}]},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","import json\n","from datetime import datetime\n","import os\n","import time\n","import logging\n","\n","class iThomeNewsScraper:\n","    def __init__(self, debug=False):\n","        # 設定請求標頭，模擬瀏覽器行為\n","        self.headers = {\n","            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n","            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n","            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',\n","            'Connection': 'keep-alive',\n","        }\n","\n","        # 設定日誌\n","        self.setup_logging(debug)\n","\n","    def setup_logging(self, debug):\n","        \"\"\"設定日誌系統\"\"\"\n","        level = logging.DEBUG if debug else logging.INFO\n","        logging.basicConfig(\n","            level=level,\n","            format='%(asctime)s - %(levelname)s - %(message)s',\n","            handlers=[\n","                logging.FileHandler('scraper.log', encoding='utf-8'),\n","                logging.StreamHandler()\n","            ]\n","        )\n","        self.logger = logging.getLogger(__name__)\n","\n","    def get_article_content(self, url):\n","        \"\"\"\n","        爬取指定URL的新聞內容\n","\n","        Args:\n","            url (str): 新聞文章的URL\n","\n","        Returns:\n","            dict: 包含新聞資訊的字典\n","        \"\"\"\n","        try:\n","            # 發送GET請求獲取網頁內容\n","            self.logger.info(f\"開始爬取文章: {url}\")\n","            response = requests.get(url, headers=self.headers, timeout=10)\n","            response.raise_for_status()\n","            response.encoding = 'utf-8'\n","\n","            # 使用BeautifulSoup解析HTML\n","            soup = BeautifulSoup(response.text, 'html.parser')\n","\n","            # 使用更靈活的選擇器並加入錯誤處理\n","            title = self._safe_get_text(soup, ['h1.page-header', 'h1.title'])\n","            if not title:\n","                raise ValueError(\"無法找到文章標題\")\n","\n","            author = self._safe_get_text(soup, ['div.author a', 'div.created a', 'span.author'])\n","            publish_time = self._safe_get_text(soup, ['div.created', 'div.meta-created'])\n","\n","            # 取得文章內容，使用多個可能的選擇器\n","            content_selectors = [\n","                'div.field-items div.field-item p',\n","                'div.article-content p',\n","                'div.content p'\n","            ]\n","            content = []\n","            for selector in content_selectors:\n","                paragraphs = soup.select(selector)\n","                if paragraphs:\n","                    content = [p.text.strip() for p in paragraphs if p.text.strip()]\n","                    break\n","\n","            if not content:\n","                self.logger.warning(f\"無法找到文章內容，嘗試使用備用方法\")\n","                # 備用方法：尋找所有段落\n","                content = [p.text.strip() for p in soup.find_all('p') if p.text.strip()]\n","\n","            article_data = {\n","                'title': title,\n","                'author': author,\n","                'publish_time': publish_time,\n","                'content': '\\n'.join(content),\n","                'url': url,\n","                'scrape_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","            }\n","\n","            # 儲存圖片URL\n","            image_selectors = [\n","                'div.field-items div.field-item img',\n","                'div.article-content img',\n","                'div.content img'\n","            ]\n","            images = []\n","            for selector in image_selectors:\n","                imgs = soup.select(selector)\n","                if imgs:\n","                    images.extend([img['src'] for img in imgs if 'src' in img.attrs])\n","\n","            article_data['images'] = images\n","\n","            self.logger.info(f\"成功爬取文章: {title}\")\n","            return article_data\n","\n","        except requests.exceptions.RequestException as e:\n","            self.logger.error(f\"請求錯誤: {str(e)}\")\n","            return None\n","        except ValueError as e:\n","            self.logger.error(f\"解析錯誤: {str(e)}\")\n","            return None\n","        except Exception as e:\n","            self.logger.error(f\"未預期的錯誤: {str(e)}\")\n","            return None\n","\n","    def _safe_get_text(self, soup, selectors):\n","        \"\"\"安全地從多個可能的選擇器中獲取文字內容\"\"\"\n","        for selector in selectors:\n","            element = soup.select_one(selector)\n","            if element and element.text:\n","                return element.text.strip()\n","        return \"\"\n","\n","    def save_to_json(self, data, filename):\n","        \"\"\"將爬取的資料儲存為JSON檔案\"\"\"\n","        try:\n","            os.makedirs('output', exist_ok=True)\n","            output_path = os.path.join('output', filename)\n","            with open(output_path, 'w', encoding='utf-8') as f:\n","                json.dump(data, f, ensure_ascii=False, indent=2)\n","            self.logger.info(f\"資料已儲存至 {output_path}\")\n","        except Exception as e:\n","            self.logger.error(f\"儲存檔案時發生錯誤: {str(e)}\")\n","\n","def main():\n","    # 啟用除錯模式\n","    scraper = iThomeNewsScraper(debug=True)\n","\n","    # 測試多個URL\n","    urls = [\n","        \"https://www.ithome.com.tw/news/152373\",\n","        \"https://www.ithome.com.tw/news/159391\"\n","    ]\n","\n","    for url in urls:\n","        try:\n","            article_data = scraper.get_article_content(url)\n","            if article_data:\n","                filename = f\"{article_data['title'][:30].replace('/', '_')}.json\"\n","                scraper.save_to_json(article_data, filename)\n","            # 加入延遲，避免過於頻繁的請求\n","            time.sleep(2)\n","        except Exception as e:\n","            logging.error(f\"爬取文章 {url} 時發生錯誤: {str(e)}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"uRf1g2B3q_RW","executionInfo":{"status":"ok","timestamp":1733282561026,"user_tz":-480,"elapsed":5880,"user":{"displayName":"呂彥欣","userId":"13178541323415113438"}}},"execution_count":5,"outputs":[]}]}